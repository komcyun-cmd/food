import streamlit as st
import pandas as pd
import time
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.common.by import By
from bs4 import BeautifulSoup
import os

# --- [ì„¤ì •] í˜ì´ì§€ ê¸°ë³¸ ì„¤ì • ---
st.set_page_config(page_title="ë‚˜ë§Œì˜ ì°ë§›ì§‘ íƒìƒ‰ê¸°", page_icon="ğŸ½ï¸", layout="wide")

# --- [í•¨ìˆ˜] í¬ë¡¤ë§ ë“œë¼ì´ë²„ ì„¤ì • (ê°€ì¥ ì¤‘ìš”!) ---
def get_driver():
    chrome_options = Options()
    chrome_options.add_argument("--headless")  # í™”ë©´ ì—†ì´ ì‹¤í–‰ (ì„œë²„ í•„ìˆ˜)
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--disable-gpu")
    
    # ë´‡ íƒì§€ ìš°íšŒ
    chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36")
    
    # Streamlit Cloud í™˜ê²½ vs ë¡œì»¬ í™˜ê²½ êµ¬ë¶„
    try:
        # Streamlit Cloud ë“±ì˜ ë¦¬ëˆ…ìŠ¤ í™˜ê²½
        service = Service("/usr/bin/chromedriver")
        driver = webdriver.Chrome(service=service, options=chrome_options)
    except:
        # ë¡œì»¬(ë‚´ PC) í™˜ê²½
        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=chrome_options)
        
    return driver

# --- [í•¨ìˆ˜] ì¹´ì¹´ì˜¤ë§µ í¬ë¡¤ë§ ë¡œì§ ---
# ì„±ëŠ¥ì„ ìœ„í•´ @st.cache_dataë¥¼ ì‚¬ìš©í•˜ì—¬ ë™ì¼ ê²€ìƒ‰ì–´ëŠ” ì¬í¬ë¡¤ë§ ë°©ì§€
@st.cache_data(show_spinner=False) 
def scrape_kakao(keyword, max_pages=3):
    data = []
    driver = get_driver()
    
    try:
        driver.get("https://map.kakao.com/")
        time.sleep(1)
        
        # ê²€ìƒ‰
        search_area = driver.find_element(By.ID, "search.keyword.query")
        search_area.send_keys(keyword)
        time.sleep(1) # ì…ë ¥ ì•ˆì •í™”
        driver.find_element(By.ID, "search.keyword.submit").click()
        time.sleep(2)
        
        # 'ì¥ì†Œ ë”ë³´ê¸°' í´ë¦­
        try:
            more_btn = driver.find_element(By.ID, "info.search.place.more")
            driver.execute_script("arguments[0].click();", more_btn)
            time.sleep(1)
        except:
            pass # ê²°ê³¼ê°€ ì ìŒ
            
        # í˜ì´ì§€ ìˆœíšŒ
        for page_idx in range(1, max_pages + 1):
            try:
                # í˜ì´ì§€ ë²ˆí˜¸ í´ë¦­
                page_btn = driver.find_element(By.ID, f"info.search.page.no{page_idx}")
                driver.execute_script("arguments[0].click();", page_btn)
                time.sleep(1.5)
                
                # BS4 íŒŒì‹±
                html = driver.page_source
                soup = BeautifulSoup(html, 'html.parser')
                place_list = soup.select('.PlaceItem')
                
                for place in place_list:
                    try:
                        name = place.select_one('.link_name').text.strip()
                        try:
                            score = float(place.select_one('.rating > .score > em').text)
                        except:
                            score = 0.0
                        try:
                            review_cnt = int(place.select_one('.rating > .review > em').text.replace(",",""))
                        except:
                            review_cnt = 0
                        addr = place.select_one('.addr').text.strip()
                        cat = place.select_one('.subcategory').text.strip()
                        link = place.select_one('.link_name')['href']
                        
                        data.append([name, score, review_cnt, cat, addr, link])
                    except:
                        continue
            except:
                break # í˜ì´ì§€ ë
                
    except Exception as e:
        st.error(f"í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    finally:
        driver.quit()
        
    df = pd.DataFrame(data, columns=['ì‹ë‹¹ëª…', 'ë³„ì ', 'ë¦¬ë·°ìˆ˜', 'ì¹´í…Œê³ ë¦¬', 'ì£¼ì†Œ', 'ë§í¬'])
    return df

# --- [UI] ìŠ¤íŠ¸ë¦¼ë¦¿ í™”ë©´ êµ¬ì„± ---
st.title("ğŸ•µï¸â€â™€ï¸ ë‚˜ë§Œì˜ ì°ë§›ì§‘ íƒìƒ‰ê¸° (Zero-Cost)")

with st.sidebar:
    st.header("ê²€ìƒ‰ ì„¤ì •")
    region = st.text_input("ê²€ìƒ‰ì–´ (ì˜ˆ: ëŒ€ì „ ìœ ì„±êµ¬ ë§›ì§‘)", value="ëŒ€ì „ ìœ ì„±êµ¬ ë§›ì§‘")
    page_limit = st.slider("ìˆ˜ì§‘í•  í˜ì´ì§€ ìˆ˜", 1, 5, 2)
    
    st.divider()
    st.markdown("### ğŸ” í•„í„°ë§ ê¸°ì¤€")
    min_score = st.slider("ìµœì†Œ ë³„ì ", 0.0, 5.0, 3.5)
    min_reviews = st.slider("ìµœì†Œ ë¦¬ë·° ìˆ˜", 0, 300, 10)
    
    run_btn = st.button("ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘! ğŸš€")

if run_btn:
    with st.status("ë°ì´í„°ë¥¼ ëª¨ìœ¼ê³  ìˆìŠµë‹ˆë‹¤... (ì•½ 10~20ì´ˆ ì†Œìš”)", expanded=True) as status:
        st.write("ğŸŒ ë¸Œë¼ìš°ì € ì‹¤í–‰ ì¤‘...")
        df = scrape_kakao(region, page_limit)
        st.write("âœ… ìˆ˜ì§‘ ì™„ë£Œ! ë°ì´í„° ì •ì œ ì¤‘...")
        status.update(label="ë¶„ì„ ì™„ë£Œ!", state="complete", expanded=False)
        
    if not df.empty:
        # í•„í„°ë§ ì ìš©
        filtered_df = df[
            (df['ë³„ì '] >= min_score) & 
            (df['ë¦¬ë·°ìˆ˜'] >= min_reviews)
        ].sort_values(by='ë³„ì ', ascending=False)
        
        st.subheader(f"ğŸ“Š '{region}' ë¶„ì„ ê²°ê³¼: {len(filtered_df)}ê³³ ë°œê²¬")
        
        # ë°ì´í„°í”„ë ˆì„ í‘œì‹œ (ë§í¬ í´ë¦­ ê°€ëŠ¥í•˜ê²Œ ì„¤ì •)
        st.dataframe(
            filtered_df,
            column_config={
                "ë§í¬": st.column_config.LinkColumn("ì§€ë„ ë³´ê¸°")
            },
            use_container_width=True
        )
        
        # ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
        st.download_button(
            label="CSVë¡œ ë‹¤ìš´ë¡œë“œ",
            data=filtered_df.to_csv(index=False).encode('utf-8-sig'),
            file_name=f"{region}_ë§›ì§‘.csv",
            mime='text/csv'
        )
    else:
        st.warning("ë°ì´í„°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ê²€ìƒ‰ì–´ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")

else:
    st.info("ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ê³  'ì‹œì‘' ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
